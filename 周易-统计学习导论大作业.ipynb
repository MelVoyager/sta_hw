{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690b458a-a548-41f5-a458-119f4546841f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 提取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528671b-faed-4d41-a78f-b16d5a757751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8139324-33eb-4439-8290-9c26c433d1c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a286b-51f4-47c2-bf6d-4e60cd9224b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# —— 1. 强制全程在 CPU —— \n",
    "device = torch.device(\"cpu\")\n",
    "print(f\" 当前设备强制为：{device}\")\n",
    "\n",
    "# 加载 logits 文件到 CPU\n",
    "ans_list, all_step_probs = torch.load(\n",
    "    \"E:/math500_logits_llama_500.pt\",\n",
    "    map_location=\"cpu\"\n",
    ")\n",
    "\n",
    "# 明确 all_step_probs 保持在 CPU\n",
    "all_step_probs = all_step_probs.to(\"cpu\")\n",
    "print(f\"all_step_probs device: {all_step_probs.device}\")\n",
    "\n",
    "# —— 2. 加载 CSV 标签，并转成 CPU tensor —— \n",
    "csv_path = \"E:/math500_500_llama.pt.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# verdict -> 0/1\n",
    "df['label'] = df['verdict'].str.lower().map({'correct': 1, 'incorrect': 0})\n",
    "\n",
    "# 构建 label tensor，并放到 CPU\n",
    "label_tensor = torch.tensor(\n",
    "    df['label'].values,\n",
    "    dtype=torch.float32\n",
    ").unsqueeze(1)  # 默认就在 CPU\n",
    "\n",
    "print(f\"label_tensor device: {label_tensor.device}\")\n",
    "\n",
    "# —— 3. 拼接 logits 和 label —— \n",
    "assert all_step_probs.shape[0] == label_tensor.shape[0], \"样本数量不一致，不能拼接！\"\n",
    "combined = torch.cat([all_step_probs, label_tensor], dim=1)\n",
    "\n",
    "print(f\" Combined shape: {combined.shape}\")\n",
    "print(f\" Combined device: {combined.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48f7de-820b-4ef9-9e5f-278d88ccc919",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737c5d9-58c5-4ef1-9098-54ea25237eae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# —— 1. 强制全程在 GPU，如果可用 —— \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" 当前设备：{device}\")\n",
    "\n",
    "# 加载 logits 文件到 GPU（注意 map_location 设置为 device）\n",
    "ans_list, all_step_probs = torch.load(\n",
    "    \"/mnt/e/gsm8k_logits_llama_7473.pt\",\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "# 明确将 all_step_probs 放到 GPU\n",
    "all_step_probs = all_step_probs.to(device)\n",
    "print(f\"all_step_probs device: {all_step_probs.device}\")\n",
    "\n",
    "# —— 2. 加载 CSV 标签，并转成 GPU tensor —— \n",
    "csv_path = \"/mnt/e/gsm8k_7473_llama.pt.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# verdict -> 0/1\n",
    "df['label'] = df['verdict'].str.lower().map({'correct': 1, 'incorrect': 0})\n",
    "\n",
    "# 构建 label tensor，并放到 GPU\n",
    "label_tensor = torch.tensor(\n",
    "    df['label'].values,\n",
    "    dtype=torch.float32\n",
    ").unsqueeze(1).to(device)\n",
    "\n",
    "print(f\"label_tensor device: {label_tensor.device}\")\n",
    "\n",
    "# —— 3. 拼接 logits 和 label —— \n",
    "assert all_step_probs.shape[0] == label_tensor.shape[0], \"样本数量不一致，不能拼接！\"\n",
    "combined = torch.cat([all_step_probs, label_tensor], dim=1)\n",
    "\n",
    "print(f\" Combined shape: {combined.shape}\")\n",
    "print(f\" Combined device: {combined.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a25a73-c6ee-43de-bfbb-f9c033f98b55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc2850-d0d4-4c9a-9cd3-a3b32a874ec1",
   "metadata": {},
   "source": [
    "### 方法1：根据top_10words重建词表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a606221-bff7-4d61-bb8b-c578676d2522",
   "metadata": {},
   "source": [
    "#### CPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2adfdf-16b3-412b-a3a2-40d958adba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def extract_top_k_vocab(X, vocab_list, top_k):\n",
    "    \"\"\"\n",
    "    对每行选出 top_k 最大值对应的列索引，合并去重后构造新的词表和矩阵。\n",
    "\n",
    "    参数：\n",
    "        X: 2D 矩阵，支持 numpy array 或 torch.Tensor，形状 [N, V]\n",
    "        vocab_list: 原始词表，长度为 V\n",
    "        top_k: 每行保留前 top_k 高概率的列索引\n",
    "\n",
    "    返回：\n",
    "        new_vocab_list: 精简后的词表（去重后的 top-k 所在列）\n",
    "        new_X: 新的 NumPy 矩阵，仅包含 new_vocab_list 对应的列\n",
    "    \"\"\"\n",
    "    # 转换为 NumPy 矩阵\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X_np = X.detach().cpu().numpy()\n",
    "    else:\n",
    "        X_np = np.asarray(X)\n",
    "\n",
    "    # 每行取 top_k 最大值索引\n",
    "    topk_idx_matrix = np.argpartition(-X_np, kth=top_k - 1, axis=1)[:, :top_k]\n",
    "\n",
    "    # 合并所有行的索引，并去重排序\n",
    "    unique_indices = np.unique(topk_idx_matrix.flatten())\n",
    "    unique_indices_sorted = np.sort(unique_indices)\n",
    "\n",
    "    # 构建新词表\n",
    "    new_vocab_list = [vocab_list[i] for i in unique_indices_sorted]\n",
    "\n",
    "    # 构建新矩阵：保留这些列\n",
    "    new_X = X_np[:, unique_indices_sorted]\n",
    "\n",
    "    return new_vocab_list, new_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e78ba-d0d7-4e72-a8e6-9184611d4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_step_probs\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "vocab_list = [f\"word_{i}\" for i in range(X.shape[1])]\n",
    "new_vocab_list, new_X = extract_top_k_vocab(X, vocab_list, top_k=10)\n",
    "\n",
    "print(\"原始词表大小：\", len(vocab_list))\n",
    "print(\"新词表大小：\", len(new_vocab_list))\n",
    "print(\"新矩阵形状：\", new_X.shape)\n",
    "\n",
    "X=new_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c92457-bc47-4ef8-814c-cc5105316a97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6f553-71b1-4789-8d4a-a2f508d4f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_top_k_vocab_gpu(X, vocab_list, top_k):\n",
    "    \"\"\"\n",
    "    在 GPU 上运行的提取 top-k 词表版本。\n",
    "    \n",
    "    参数：\n",
    "        X: torch.Tensor, shape [N, V]，应在 GPU 上\n",
    "        vocab_list: 原始词表，长度为 V\n",
    "        top_k: 每行保留前 top_k 高概率 token\n",
    "\n",
    "    返回：\n",
    "        new_vocab_list: 精简后的词表（去重的 top-k token 列索引）\n",
    "        new_X: 精简后的 torch.Tensor，仅包含选中的列（仍在 GPU 上）\n",
    "    \"\"\"\n",
    "    assert isinstance(X, torch.Tensor), \"X 必须是 torch.Tensor\"\n",
    "    assert X.is_cuda, \"X 必须在 GPU 上\"\n",
    "\n",
    "    # 每行取 top_k 最大值的列索引\n",
    "    topk_indices = torch.topk(X, k=top_k, dim=1).indices  # shape: [N, top_k]\n",
    "\n",
    "    # Flatten 并去重\n",
    "    unique_indices = torch.unique(topk_indices)\n",
    "    unique_indices_sorted, _ = torch.sort(unique_indices)\n",
    "\n",
    "    # 构建新词表\n",
    "    new_vocab_list = [vocab_list[i] for i in unique_indices_sorted.tolist()]\n",
    "\n",
    "    # 按列索引提取新矩阵\n",
    "    new_X = X[:, unique_indices_sorted]\n",
    "\n",
    "    return new_vocab_list, new_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2344a3-398b-4e34-86eb-55245963f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 all_step_probs 已经在 GPU 上\n",
    "X = all_step_probs  # shape [N, V]\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "# 原始词表\n",
    "vocab_list = [f\"word_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# 提取 GPU 上 top-k 精简词表和矩阵\n",
    "new_vocab_list, new_X = extract_top_k_vocab_gpu(X, vocab_list, top_k=10)\n",
    "\n",
    "print(\"原始词表大小：\", len(vocab_list))\n",
    "print(\"新词表大小：\", len(new_vocab_list))\n",
    "print(\"新矩阵形状：\", new_X.shape)\n",
    "\n",
    "# 更新 X\n",
    "X = new_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130c4ea-a710-41db-9dd5-e6c4cd0ed6f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a42352-f8ed-48db-8a30-a8c95b73d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 随机划分：80% 训练集，20% 测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,       # 测试集占 20%\n",
    "    random_state=42,     # 固定随机种子，保证可复现\n",
    ")\n",
    "\n",
    "print(f\"训练集样本数: {X_train.shape[0]}，测试集样本数: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c370aa-6088-4cc6-a565-3c6159046640",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 逻辑回归+L2 penalty+加权处理类别不平衡（降维后）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31409e28-a47e-4575-ba08-f978772a0b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# —— 0. joblib 临时目录（可选） —— \n",
    "joblib_tmp_dir = \"D:/joblib_temp\"\n",
    "os.makedirs(joblib_tmp_dir, exist_ok=True)\n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = joblib_tmp_dir\n",
    "tempfile.tempdir = joblib_tmp_dir\n",
    "\n",
    "# —— 1. 定义模型 & GridSearchCV —— \n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "param_grid = {'C': C_values, 'penalty': ['l2'], 'solver': ['saga']}\n",
    "base_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# —— 2. 确保数据在 CPU 上的 NumPy —— \n",
    "def to_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.cpu().numpy()\n",
    "    return x\n",
    "\n",
    "X_train_np = to_numpy(X_train)\n",
    "y_train_np = to_numpy(y_train)\n",
    "X_test_np  = to_numpy(X_test)\n",
    "y_test_np  = to_numpy(y_test)\n",
    "\n",
    "# —— 3. 训练 —— \n",
    "print(\"开始在 CPU 上训练 GridSearchCV …\")\n",
    "grid.fit(X_train_np, y_train_np)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "print(\" Best params:\", grid.best_params_)\n",
    "print(\"交叉验证最佳准确率：\", grid.best_score_)\n",
    "\n",
    "# —— 4. 使用全集进行评估 —— \n",
    "X_np = to_numpy(X)\n",
    "y_np = to_numpy(y)\n",
    "\n",
    "# 默认阈值 0.5\n",
    "y_pred_05 = best_model.predict(X_np)\n",
    "print(\"\\n[默认阈值 0.5] Accuracy:\", accuracy_score(y_np, y_pred_05))\n",
    "print(classification_report(y_np, y_pred_05, zero_division=0))\n",
    "\n",
    "# —— 5. 阈值搜索（基于全集）—— \n",
    "probs = best_model.predict_proba(X_np)[:, 1]\n",
    "thresholds = np.arange(0.0, 1.0 + 0.001, 0.001)\n",
    "\n",
    "best_t, best_acc = 0.5, 0.0\n",
    "for t in thresholds:\n",
    "    y_pred_t = (probs >= t).astype(int)\n",
    "    acc = accuracy_score(y_np, y_pred_t)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_t = t\n",
    "\n",
    "print(f\" 最优阈值（按 Accuracy 最大化，步长=0.001）：{best_t:.3f}，对应 Accuracy={best_acc:.4f}\")\n",
    "\n",
    "# 最优阈值评估\n",
    "y_pred_opt = (probs >= best_t).astype(int)\n",
    "\n",
    "acc_opt   = accuracy_score(y_np, y_pred_opt)\n",
    "f1_opt    = f1_score(y_np, y_pred_opt, zero_division=0)\n",
    "auc_score = roc_auc_score(y_np, probs)\n",
    "\n",
    "print(\"\\n 最终指标（在最优阈值下，全体样本）：\")\n",
    "print(f\"Accuracy: {acc_opt:.4f}\")\n",
    "print(f\"F1-score: {f1_opt:.4f}\")\n",
    "print(f\"AUC:       {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecae279-b53c-4b1b-90c3-4ff0f1735cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_opt_log=acc_opt\n",
    "f1_opt_log=f1_opt\n",
    "auc_opt_log=auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff033d-b6fb-4f86-8fb6-5e7ae5599b16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ad520-c5a4-41e0-9dd2-d564b9510635",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 线性核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec8946-6b48-4a1d-8d4a-9aa1f744fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "clf = LinearSVC(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "def compute_svc_metrics(clf, X_test, y_test):\n",
    "    \"\"\"\n",
    "    计算线性 SVC 的 Accuracy、F1-score 和 AUC，并作为返回值返回。\n",
    "    \"\"\"\n",
    "    # 1. 预测标签\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # 2. 决策函数得分（用于 AUC）\n",
    "    y_scores = clf.decision_function(X_test)\n",
    "    \n",
    "    # 3. 计算指标\n",
    "    acc = accuracy_score(y_test,   y_pred)\n",
    "    f1  = f1_score(y_test,        y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test,   y_scores)\n",
    "    \n",
    "    return acc, f1, auc\n",
    "\n",
    "# —— 使用示例 —— \n",
    "acc_opt_linear, f1_opt_linear, auc_opt_linear = compute_svc_metrics(clf, X, y)\n",
    "\n",
    "print(f\"Accuracy         = {acc_opt_linear:.4f}\")\n",
    "print(f\"F1-score         = {f1_opt_linear:.4f}\")\n",
    "print(f\"AUC (ROC AUC)    = {auc_opt_linear:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b38e8-de74-4669-b615-ccbb2144c02b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 多项式核"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6821b1-b1f7-4481-bf61-6c9b2d7be038",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### CPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c3eff-7271-4f98-b91f-b5b4797d0db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "#  确保是 numpy 格式\n",
    "X_train = X_train.cpu().numpy() if isinstance(X_train, torch.Tensor) else X_train\n",
    "y_train = y_train.cpu().numpy() if isinstance(y_train, torch.Tensor) else y_train\n",
    "X_test = X_test.cpu().numpy() if isinstance(X_test, torch.Tensor) else X_test\n",
    "y_test = y_test.cpu().numpy() if isinstance(y_test, torch.Tensor) else y_test\n",
    "\n",
    "#  网格参数\n",
    "C_values = [0.01, 0.1, 1.0, 10.0]\n",
    "degrees = [2, 3, 4]\n",
    "coef0_values = [0.0, 1.0]\n",
    "param_grid = [(C, d, c0) for C in C_values for d in degrees for c0 in coef0_values]\n",
    "\n",
    "#  交叉验证\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for C, degree, coef0 in param_grid:\n",
    "    print(f\"\\n Grid Search: C={C}, degree={degree}, coef0={coef0}\")\n",
    "    acc_list, f1_list, auc_list = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        try:\n",
    "            model = SVC(\n",
    "                kernel='poly',\n",
    "                C=C,\n",
    "                degree=degree,\n",
    "                coef0=coef0,\n",
    "                probability=True,\n",
    "                random_state=42,\n",
    "                max_iter=10000\n",
    "            )\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            f1 = f1_score(y_val, y_pred, average='macro')\n",
    "            auc = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "            acc_list.append(acc)\n",
    "            f1_list.append(f1)\n",
    "            auc_list.append(auc)\n",
    "        except Exception as e:\n",
    "            print(f\"   Fold {fold+1} 出错: {e}\")\n",
    "            acc_list.append(np.nan)\n",
    "            f1_list.append(np.nan)\n",
    "            auc_list.append(np.nan)\n",
    "\n",
    "    mean_acc = np.nanmean(acc_list)\n",
    "    mean_f1 = np.nanmean(f1_list)\n",
    "    mean_auc = np.nanmean(auc_list)\n",
    "    results.append((C, degree, coef0, mean_acc, mean_f1, mean_auc))\n",
    "    print(f\" CV 平均：Acc={mean_acc:.4f}, F1={mean_f1:.4f}, AUC={mean_auc:.4f}\")\n",
    "\n",
    "#  最佳参数\n",
    "valid_results = [r for r in results if not np.isnan(r[4])]\n",
    "best = max(valid_results, key=lambda x: x[4])\n",
    "best_C, best_degree, best_coef0 = best[:3]\n",
    "\n",
    "print(\"\\n 最佳参数：C=%.3f, degree=%d, coef0=%.1f\" % (best_C, best_degree, best_coef0))\n",
    "\n",
    "#  在训练集上重训，在测试集上评估\n",
    "model_test = SVC(\n",
    "    kernel='poly',\n",
    "    C=best_C,\n",
    "    degree=best_degree,\n",
    "    coef0=best_coef0,\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "model_test.fit(X_train, y_train)\n",
    "y_pred_test = model_test.predict(X_test)\n",
    "y_proba_test = model_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "auc_test = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "print(\"\\n 测试集评估：\")\n",
    "print(f\" Accuracy = {acc_test:.4f}\")\n",
    "print(f\" F1-score = {f1_test:.4f}\")\n",
    "print(f\" AUC      = {auc_test:.4f}\")\n",
    "\n",
    "#  重新在全集上训练并输出最终评估\n",
    "X_full = np.vstack([X_train, X_test])\n",
    "y_full = np.hstack([y_train, y_test])\n",
    "\n",
    "model_final = SVC(\n",
    "    kernel='poly',\n",
    "    C=best_C,\n",
    "    degree=best_degree,\n",
    "    coef0=best_coef0,\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "model_final.fit(X_full, y_full)\n",
    "y_pred_full = model_final.predict(X_full)\n",
    "y_proba_full = model_final.predict_proba(X_full)[:, 1]\n",
    "\n",
    "acc_full_ply = accuracy_score(y_full, y_pred_full)\n",
    "f1_full_ply = f1_score(y_full, y_pred_full, average='macro')\n",
    "auc_full_ply = roc_auc_score(y_full, y_proba_full)\n",
    "\n",
    "print(\"\\n 全集最终评估：\")\n",
    "print(f\" Accuracy = {acc_full_ply:.4f}\")\n",
    "print(f\" F1-score = {f1_full_ply:.4f}\")\n",
    "print(f\" AUC      = {auc_full_ply:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43086cdc-b84b-4730-a228-d0195a54a1eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55e434-8a0e-4aa9-8d06-d12eae079308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.svm import SVC as cuSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "#  数据转换\n",
    "X_cu = cudf.DataFrame(cp.asarray(X))\n",
    "y_cu = cudf.Series(cp.asarray(y))\n",
    "\n",
    "#  训练/测试划分\n",
    "X_train_cu, X_test_cu, y_train_cu, y_test_cu = train_test_split(\n",
    "    X_cu, y_cu,\n",
    "    test_size=0.2,\n",
    "    stratify=y_cu.to_pandas(),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#  多项式核参数网格\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 1.0],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_model = None\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for C in param_grid['C']:\n",
    "    for degree in param_grid['degree']:\n",
    "        for coef0 in param_grid['coef0']:\n",
    "            try:\n",
    "                print(f\"\\n Fitting: C={C}, degree={degree}, coef0={coef0}, kernel=poly\")\n",
    "\n",
    "                model = cuSVC(C=C, degree=degree, coef0=coef0, kernel='poly', probability=False)\n",
    "                model.fit(X_train_cu, y_train_cu)\n",
    "\n",
    "                y_pred = model.predict(X_cu).values_host\n",
    "                y_scores = model.decision_function(X_cu).values_host\n",
    "                y_true = y_cu.to_pandas().values\n",
    "\n",
    "                acc = accuracy_score(y_true, y_pred)\n",
    "                f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "                results.append({'C': C, 'degree': degree, 'coef0': coef0, 'acc': acc, 'f1': f1, 'auc': auc})\n",
    "                print(f\"   acc={acc:.4f}, f1={f1:.4f}, auc={auc:.4f}\")\n",
    "\n",
    "                if f1 > best_score:\n",
    "                    best_score = f1\n",
    "                    best_model = model\n",
    "                    best_params = {'C': C, 'degree': degree, 'coef0': coef0, 'kernel': 'poly'}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error: {str(e)}\")\n",
    "            finally:\n",
    "                cp._default_memory_pool.free_all_blocks()\n",
    "                gc.collect()\n",
    "\n",
    "#  最佳模型评估\n",
    "if best_model is not None:\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    y_pred = best_model.predict(X_cu).values_host\n",
    "    y_scores = best_model.decision_function(X_cu).values_host\n",
    "    y_true = y_cu.to_pandas().values\n",
    "\n",
    "    acc_opt = accuracy_score(y_true, y_pred)\n",
    "    f1_opt = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    auc_opt = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    print(\"\\n Final Evaluation on Full Set:\")\n",
    "    print(f\"  Accuracy : {acc_opt:.4f}\")\n",
    "    print(f\"  F1-score : {f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC  : {auc_opt:.4f}\\n\")\n",
    "\n",
    "    print(\" Detailed Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "else:\n",
    "    print(\" Grid Search failed for all parameter combinations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fff55-487c-441f-a2d9-4908e772835c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RBF核"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f75d2-2ffc-4c29-ac4a-2752141782eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### CPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c6379-a09f-453c-9f3f-c04db2c1ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "#  Step 0: 转为 numpy\n",
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy() if hasattr(x, 'detach') else x\n",
    "\n",
    "X_train = to_numpy(X_train)\n",
    "y_train = to_numpy(y_train)\n",
    "X_test = to_numpy(X_test)\n",
    "y_test = to_numpy(y_test)\n",
    "\n",
    "#  Step 1: 参数网格\n",
    "C_values = [0.01, 0.1, 1.0, 10.0]\n",
    "gamma_values = ['scale', 'auto', 0.01, 0.001]\n",
    "param_grid = [(C, g) for C in C_values for g in gamma_values]\n",
    "\n",
    "#  Step 2: Grid Search with 5-fold CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "print(\" 开始 RBF Grid Search...\")\n",
    "for C, gamma in param_grid:\n",
    "    print(f\"\\n 参数组合：C={C}, gamma={gamma}\")\n",
    "    acc_list, f1_list, auc_list = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        try:\n",
    "            model = SVC(\n",
    "                kernel='rbf',\n",
    "                C=C,\n",
    "                gamma=gamma,\n",
    "                probability=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            f1 = f1_score(y_val, y_pred, average='macro')\n",
    "            auc = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "            acc_list.append(acc)\n",
    "            f1_list.append(f1)\n",
    "            auc_list.append(auc)\n",
    "\n",
    "            print(f\"  Fold {fold+1}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   Fold {fold+1} 错误: {e}\")\n",
    "            acc_list.append(np.nan)\n",
    "            f1_list.append(np.nan)\n",
    "            auc_list.append(np.nan)\n",
    "\n",
    "    mean_acc = np.nanmean(acc_list)\n",
    "    mean_f1 = np.nanmean(f1_list)\n",
    "    mean_auc = np.nanmean(auc_list)\n",
    "    results.append((C, gamma, mean_acc, mean_f1, mean_auc))\n",
    "    print(f\" CV 平均：Acc={mean_acc:.4f}, F1={mean_f1:.4f}, AUC={mean_auc:.4f}\")\n",
    "\n",
    "#  Step 3: 选出最优参数（按 F1 排序）\n",
    "valid_results = [r for r in results if not np.isnan(r[3])]\n",
    "best = max(valid_results, key=lambda x: x[3])  # F1-score\n",
    "best_C, best_gamma = best[0], best[1]\n",
    "\n",
    "print(\"\\n 最佳参数：C=%.3f, gamma=%s\" % (best_C, str(best_gamma)))\n",
    "\n",
    "#  Step 4: 在测试集上评估\n",
    "model_test = SVC(\n",
    "    kernel='rbf',\n",
    "    C=best_C,\n",
    "    gamma=best_gamma,\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "model_test.fit(X_train, y_train)\n",
    "y_pred_test = model_test.predict(X_test)\n",
    "y_proba_test = model_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "auc_test = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "print(\"\\n 测试集评估：\")\n",
    "print(f\" Accuracy = {acc_test:.4f}\")\n",
    "print(f\" F1-score = {f1_test:.4f}\")\n",
    "print(f\" AUC      = {auc_test:.4f}\")\n",
    "\n",
    "#  Step 5: 全集训练 + 评估\n",
    "X_full = np.vstack([X_train, X_test])\n",
    "y_full = np.hstack([y_train, y_test])\n",
    "\n",
    "model_full = SVC(\n",
    "    kernel='rbf',\n",
    "    C=best_C,\n",
    "    gamma=best_gamma,\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "model_full.fit(X_full, y_full)\n",
    "y_pred_full = model_full.predict(X_full)\n",
    "y_proba_full = model_full.predict_proba(X_full)[:, 1]\n",
    "\n",
    "acc_opt_rbf = accuracy_score(y_full, y_pred_full)\n",
    "f1_opt_rbf = f1_score(y_full, y_pred_full, average='macro')\n",
    "auc_opt_rbf = roc_auc_score(y_full, y_proba_full)\n",
    "\n",
    "print(\"\\n 全集最终评估：\")\n",
    "print(f\" Accuracy = {acc_opt_rbf:.4f}\")\n",
    "print(f\" F1-score = {f1_opt_rbf:.4f}\")\n",
    "print(f\" AUC      = {auc_opt_rbf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b12475-35fd-4199-9db1-d8e0ccf810aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4531e-cf95-436f-98e8-5f59f6e8091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from cuml.svm import SVC as cuSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# —— 0. 数据准备：转为 GPU —— \n",
    "X_cu = cudf.DataFrame(cp.asarray(X))\n",
    "y_cu = cudf.Series(cp.asarray(y))\n",
    "\n",
    "# —— 1. 划分训练 / 测试 —— \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cu, y_cu,\n",
    "    test_size=0.2,\n",
    "    stratify=y_cu.to_pandas(),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# —— 2. 手动 Grid Search —— \n",
    "param_grid = {\n",
    "    'C':     [0.1, 1.0, 10.0, 100.0],\n",
    "    'gamma': ['scale', 'auto'],  # cuML 支持的 gamma 有限\n",
    "    'kernel':['rbf']\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for C in param_grid['C']:\n",
    "    for gamma in param_grid['gamma']:\n",
    "        for kernel in param_grid['kernel']:\n",
    "            try:\n",
    "                print(f\" Fitting: C={C}, gamma={gamma}, kernel={kernel}\")\n",
    "                model = cuSVC(C=C, gamma=gamma, kernel=kernel)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_cu).values_host\n",
    "                y_scores = model.decision_function(X_cu).values_host\n",
    "\n",
    "                acc = accuracy_score(y, y_pred)\n",
    "                f1  = f1_score(y, y_pred, average='macro', zero_division=0)\n",
    "                auc = roc_auc_score(y, y_scores)\n",
    "\n",
    "                print(f\"    acc={acc:.4f} | f1={f1:.4f} | auc={auc:.4f}\")\n",
    "\n",
    "                if acc > best_score:\n",
    "                    best_score = acc\n",
    "                    best_model = model\n",
    "                    best_params = {'C': C, 'gamma': gamma, 'kernel': kernel}\n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {e}\")\n",
    "            finally:\n",
    "                cp._default_memory_pool.free_all_blocks()\n",
    "                gc.collect()\n",
    "\n",
    "# —— 3. 最佳模型评估 —— \n",
    "if best_model is not None:\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    y_pred = best_model.predict(X_cu).values_host\n",
    "    y_scores = best_model.decision_function(X_cu).values_host\n",
    "\n",
    "    acc_opt_rbf = accuracy_score(y, y_pred)\n",
    "    f1_opt_rbf  = f1_score(y, y_pred, average='macro', zero_division=0)\n",
    "    auc_opt_rbf = roc_auc_score(y, y_scores)\n",
    "\n",
    "    print(\"\\n Final Evaluation on Full Set:\")\n",
    "    print(f\"  Accuracy : {acc_opt_rbf:.4f}\")\n",
    "    print(f\"  F1-score : {f1_opt_rbf:.4f}\")\n",
    "    print(f\"  ROC-AUC  : {auc_opt_rbf:.4f}\\n\")\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print(classification_report(y, y_pred, zero_division=0))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "else:\n",
    "    print(\" Grid Search failed for all params.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444c903-6266-4f00-a598-01836146e037",
   "metadata": {},
   "source": [
    "## 决策树（随机森林）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd3983-ed7a-4257-bc05-e272d248b7fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d39677-2738-491c-93d9-37eb242284f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "#  确保 X_train、X_test 是 numpy.ndarray\n",
    "def to_numpy(t):\n",
    "    return t.detach().cpu().numpy() if isinstance(t, torch.Tensor) else t\n",
    "\n",
    "X_train = to_numpy(X_train)\n",
    "y_train = to_numpy(y_train)\n",
    "X_test = to_numpy(X_test)\n",
    "y_test = to_numpy(y_test)\n",
    "\n",
    "#  1. 维度信息\n",
    "p = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "#  2. max_features 候选值\n",
    "q_values = sorted(set([\n",
    "    int(np.sqrt(p)),\n",
    "    int(p ** (2 / 3)),\n",
    "    int(p * 0.3),\n",
    "    int(p * 0.5)\n",
    "]))\n",
    "q_values = [q for q in q_values if 1 <= q <= p]\n",
    "print(\" max_features 候选值:\", q_values)\n",
    "\n",
    "#  3. 交叉验证 Grid Search\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for q in q_values:\n",
    "    print(f\"\\n 正在评估 max_features = {q}\")\n",
    "    acc_list, f1_list, auc_list = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            max_features=q,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro')\n",
    "        auc = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "        print(f\"  Fold {fold+1}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "    mean_acc = np.mean(acc_list)\n",
    "    mean_f1 = np.mean(f1_list)\n",
    "    mean_auc = np.mean(auc_list)\n",
    "    results.append((q, mean_acc, mean_f1, mean_auc))\n",
    "\n",
    "    print(f\" 平均值：Acc={mean_acc:.4f}, F1={mean_f1:.4f}, AUC={mean_auc:.4f}\")\n",
    "\n",
    "#  4. 选择最优超参数\n",
    "best_q, best_acc, best_f1, best_auc = max(results, key=lambda x: x[2])\n",
    "print(\"\\n 最优 max_features:\", best_q)\n",
    "print(f\" CV Accuracy = {best_acc:.4f}, F1 = {best_f1:.4f}, AUC = {best_auc:.4f}\")\n",
    "\n",
    "#  5. 在测试集上评估\n",
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    max_features=best_q,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "y_proba_test = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "auc_test = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "print(\"\\n 测试集评估结果：\")\n",
    "print(f\" Accuracy = {acc_test:.4f}\")\n",
    "print(f\" F1-score = {f1_test:.4f}\")\n",
    "print(f\" AUC      = {auc_test:.4f}\")\n",
    "\n",
    "#  6. 全集训练并评估（训练 + 测试集合并）\n",
    "X_full = np.vstack([X_train, X_test])\n",
    "y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "final_model_full = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    max_features=best_q,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_model_full.fit(X_full, y_full)\n",
    "y_pred_full = final_model_full.predict(X_full)\n",
    "y_proba_full = final_model_full.predict_proba(X_full)[:, 1]\n",
    "\n",
    "acc_opt_rf = accuracy_score(y_full, y_pred_full)\n",
    "f1_opt_rf = f1_score(y_full, y_pred_full, average='macro')\n",
    "auc_opt_rf = roc_auc_score(y_full, y_proba_full)\n",
    "\n",
    "print(\"\\n 全集最终评估结果：\")\n",
    "print(f\" Accuracy = {acc_opt_rf:.4f}\")\n",
    "print(f\" F1-score = {f1_opt_rf:.4f}\")\n",
    "print(f\" AUC      = {auc_opt_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65141b0-fbb2-44cd-8910-b135737aae0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GPU版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27baa45c-66c4-405d-abd4-e3cb39db302b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torch\n",
    "\n",
    "#  你已有的 X 和 y：应为 torch.cuda.Tensor\n",
    "# 确保已加载\n",
    "# X: (n_samples, n_features)\n",
    "# y: (n_samples,)\n",
    "\n",
    "# 1️ 特征维度和类别数\n",
    "p = X.shape[1]\n",
    "y_cpu_np = y.cpu().numpy() if isinstance(y, torch.Tensor) else y\n",
    "n_classes = len(np.unique(y_cpu_np))\n",
    "\n",
    "# 2️ max_features 候选值\n",
    "q_values = sorted(set([\n",
    "    int(np.sqrt(p)),\n",
    "    int(p ** (2 / 3)),\n",
    "    int(p * 0.3),\n",
    "    int(p * 0.5)\n",
    "]))\n",
    "q_values = [q for q in q_values if 1 <= q <= p]\n",
    "print(\" max_features 候选值:\", q_values)\n",
    "\n",
    "# 3️ 转为 cuDF（全程保留在 GPU）\n",
    "X_cudf = cudf.DataFrame(cp.asarray(X))\n",
    "y_cudf = cudf.Series(cp.asarray(y))\n",
    "\n",
    "# 4️ Stratified CV 在 CPU 上生成索引\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 5️ 交叉验证过程\n",
    "results = []\n",
    "for q in q_values:\n",
    "    print(f\"\\n 正在评估 max_features = {q}\")\n",
    "    acc_list, f1_list, auc_list = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_cpu_np)), y_cpu_np)):\n",
    "        X_train = X_cudf.iloc[train_idx]\n",
    "        X_val   = X_cudf.iloc[val_idx]\n",
    "        y_train = y_cudf.iloc[train_idx]\n",
    "        y_val   = y_cudf.iloc[val_idx]\n",
    "\n",
    "        model = cuRF(n_estimators=100, max_depth=10, max_features=q, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_proba = model.predict_proba(X_val)\n",
    "\n",
    "        # 转为 numpy（在 CPU 上做 sklearn 评估）\n",
    "        y_true = y_val.to_numpy()\n",
    "        y_pred_np = y_pred.to_numpy()\n",
    "        y_proba_np = y_proba.to_numpy()\n",
    "\n",
    "        acc = float(accuracy_score(y_val, y_pred))\n",
    "        f1 = f1_score(y_true, y_pred_np, average='macro')\n",
    "\n",
    "        #  补全 AUC 维度（处理类别缺失情况）\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        y_pred_proba_full = np.zeros((y_proba_np.shape[0], n_classes))\n",
    "        y_present_classes = np.unique(y_true)\n",
    "        for i, cls in enumerate(y_present_classes):\n",
    "            y_pred_proba_full[:, cls] = y_proba_np[:, i]\n",
    "\n",
    "        auc = roc_auc_score(y_true, y_proba_np[:, 1])\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "        print(f\"  Fold {fold+1}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "    mean_acc = np.mean(acc_list)\n",
    "    mean_f1 = np.mean(f1_list)\n",
    "    mean_auc = np.mean(auc_list)\n",
    "    results.append((q, mean_acc, mean_f1, mean_auc))\n",
    "\n",
    "    print(f\" 平均值：Acc={mean_acc:.4f}, F1={mean_f1:.4f}, AUC={mean_auc:.4f}\")\n",
    "\n",
    "# 6️ 选择最佳 max_features（按 F1 排名）\n",
    "best_q, best_acc, best_f1, best_auc = max(results, key=lambda x: x[2])\n",
    "print(\"\\n 最优 max_features:\", best_q)\n",
    "print(f\" 最佳 Accuracy        : {best_acc:.4f}\")\n",
    "print(f\" 最佳 F1-score (macro): {best_f1:.4f}\")\n",
    "print(f\" 最佳 AUC     (macro): {best_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba19fdd-7eb9-4ae7-88cb-90208a69670f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 两层MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604079c-0a64-40c3-9075-80153ea52b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1️ 数据准备\n",
    "# 若为稀疏矩阵，先转为稠密\n",
    "X_train_tensor = torch.tensor(X_train.toarray() if hasattr(X_train, 'toarray') else X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.toarray() if hasattr(X_test, 'toarray') else X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 2️ 定义两层 MLP\n",
    "class TwoLayerMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # 输出两个类\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = TwoLayerMLP(input_dim=X_train_tensor.shape[1])\n",
    "\n",
    "# 3️ 训练配置\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# 4️ 批训练\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5️ 预测与评估\n",
    "#  用全集数据测试（X_all = X_train + X_test）\n",
    "X_all_tensor = torch.cat([X_train_tensor, X_test_tensor], dim=0)\n",
    "y_all_tensor = torch.cat([y_train_tensor, y_test_tensor], dim=0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_all = model(X_all_tensor)\n",
    "    y_proba_all = torch.softmax(logits_all, dim=1)[:, 1].cpu().numpy()  # 正类概率\n",
    "    y_pred_all = torch.argmax(logits_all, dim=1).cpu().numpy()\n",
    "    y_true_all = y_all_tensor.cpu().numpy()\n",
    "\n",
    "#  计算全集上的指标\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "acc_all = accuracy_score(y_true_all, y_pred_all)\n",
    "f1_all = f1_score(y_true_all, y_pred_all, average='macro', zero_division=0)\n",
    "auc_all = roc_auc_score(y_true_all, y_proba_all)\n",
    "\n",
    "print(\" Evaluation on the Entire Dataset\")\n",
    "print(f\"Accuracy: {acc_all:.4f}\")\n",
    "print(f\"F1-score: {f1_all:.4f}\")\n",
    "print(f\"AUC:      {auc_all:.4f}\")\n",
    "print(classification_report(y_true_all, y_pred_all, digits=4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true_all, y_pred_all))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b58772-9649-4777-af59-7ea758d56c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 绘图部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b422d9-ecb0-4d3c-8f12-7d885b70e7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 模拟数据（请将这些值替换为你真实的结果）\n",
    "methods = [\"Logistic\", \"Linear-SVM\",  \"Poly-SVM\",\"RBF-SVM\", \"Random Forest\",\"MLP\"]\n",
    "f1_scores = [f1_opt_log, f1_opt_linear,  f1_full_ply, f1_opt_rbf,f1_opt_rf,f1_all]\n",
    "auc_scores = [auc_opt_log,  auc_opt_linear, auc_full_ply,auc_opt_rbf,auc_opt_rf, auc_all]\n",
    "acc_scores = [acc_opt_log,  acc_opt_linear, acc_full_ply,acc_opt_rbf,accuracy_opt_rf, acc_all]\n",
    "\n",
    "# 设置柱状图参数\n",
    "x = np.arange(len(methods))  # 每组的X位置\n",
    "width = 0.25  # 每个柱子的宽度\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width, f1_scores, width, label='F1-score', color='#6D749E')   # 蓝色\n",
    "rects2 = ax.bar(x, auc_scores, width, label='AUC', color='#D8B7A6')               # 红色\n",
    "rects3 = ax.bar(x + width, acc_scores, width, label='Accuracy', color='#545E57')  # 黑色\n",
    "\n",
    "# 添加标签、标题和图例\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# 显示每个柱子的数值\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 2),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存为 JPG 文件\n",
    "output_path = \"E:/tongji/gsm8k_qwen_graph1.jpg\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9d769-2c83-4c0b-a85e-aaf1553a1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_scores)\n",
    "print(auc_scores)\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a75898-a352-46c8-aa0c-92291f862204",
   "metadata": {},
   "source": [
    "## 方法2：使用truncated SVD去简化词表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf44dc-be76-4ea1-bfe9-5fcb279c97a2",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3dc16-736e-4904-9650-37684d79919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 all_step_probs 是你已经加载好的 numpy 数组\n",
    "# shape = (7374, 152064)\n",
    "\n",
    "# 1. 保留前 128000 列\n",
    "X_main = all_step_probs[:, :128000]\n",
    "\n",
    "# 2. 合并不同区段\n",
    "part1 = all_step_probs[:, 128000:128798].sum(dim=1, keepdim=True)\n",
    "part2 = all_step_probs[:, 128798:128800].sum(dim=1, keepdim=True)\n",
    "part3 = all_step_probs[:, 128814:128815]\n",
    "part4 = all_step_probs[:, 128815:].sum(dim=1, keepdim=True)\n",
    "\n",
    "X = torch.cat([X_main, part1, part2, part3, part4], dim=1)\n",
    "print(\"压缩后形状：\", X.shape)\n",
    "\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "X_1 = X[y == 1]\n",
    "X_0 = X[y == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33bb37-442f-44b7-9197-4bf9747577f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def select_k_elbow(X, max_k=200, random_state=0, save_path=\"E:/tongji/truncated_svd.jpg\",\n",
    "                   line_color=\"#6D749E\",  # 蓝色主线\n",
    "                   marker_color=\"#D8B7A6\",  # 橙色点\n",
    "                   elbow_line_color=\"#545E57\"  # 绿色垂直线\n",
    "                  ):\n",
    "    svd = TruncatedSVD(n_components=max_k, random_state=random_state)\n",
    "    svd.fit(X)\n",
    "\n",
    "    cum_var = np.cumsum(svd.explained_variance_ratio_)\n",
    "    diffs = np.diff(cum_var)\n",
    "    elbow_k = 6\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # 主线\n",
    "    plt.plot(np.arange(1, max_k + 1), cum_var, color=line_color,\n",
    "             marker='o', markerfacecolor=marker_color, linewidth=1, label='Cumulative Variance')\n",
    "    \n",
    "    # 垂直线\n",
    "    plt.axvline(elbow_k, color=elbow_line_color, linestyle='--', label=f'Elbow at k={elbow_k}')\n",
    "    \n",
    "    plt.xlabel('n_components (k)')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Elbow Method for TruncatedSVD')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"图像已保存到：{save_path}\")\n",
    "    return elbow_k, cum_var, svd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9dcb7-90fc-446e-ae1a-748c4aecdf1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elbow_k, cum_var, svd = select_k_elbow(X, max_k=100,   save_path = \"E:/tongji/truncated_svd.jpg\")\n",
    "elbow_k_1, cum_var_1, svd_1 = select_k_elbow(X_1, max_k=100,   save_path = \"E:/tongji/truncated_svd_1.jpg\")\n",
    "elbow_k_0, cum_var_0, svd_0 = select_k_elbow(X_0, max_k=100,   save_path = \"E:/tongji/truncated_svd_0.jpg\")\n",
    "\n",
    "vocab_list = [f\"word_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "#vocab_list = [f\"word_{i}\" for i in range(X.shape[1])]\n",
    "#new_vocab_list, new_X = extract_top_p_vocab(X, vocab_list, threshold=0.9)\n",
    "\n",
    "print(f\"原始词表大小: {len(vocab_list)}\")\n",
    "print(f\"新的精简词表大小: {elbow_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef20ce-6d0b-4cee-9487-4111b4ac1d08",
   "metadata": {},
   "source": [
    "### 处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43aebff-ac20-432e-88a5-e9c8284aef90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 提取第一主成分向量（长度 = vocab size）\n",
    "first_component = svd.components_[0]  # shape: (n_features,)\n",
    "print(\"第一主成分向量长度：\", len(first_component))\n",
    "\n",
    "# 获取绝对值最大的前 N 个 token（代表这个主成分的核心方向）\n",
    "N = 20\n",
    "sorted_indices = np.argsort(np.abs(first_component))[::-1][:N]\n",
    "top_tokens = [(vocab_list[i], first_component[i]) for i in sorted_indices]\n",
    "\n",
    "print(f\"\\n第一主成分中权重最大的前 {N} 个 token：\")\n",
    "for token, weight in top_tokens:\n",
    "    print(f\"{token}\\t{weight:.10f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507fadb-d268-491d-acfc-3f7731e2a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20  # 每个主成分取前 N 个权重最大 token\n",
    "num_components = 6\n",
    "\n",
    "for k in range(num_components):\n",
    "    component = svd.components_[k]  # 第 k 个主成分向量\n",
    "    sorted_indices = np.argsort(np.abs(component))[::-1][:N]\n",
    "    \n",
    "    print(f\"\\n 第 {k+1} 个主成分中权重最大的前 {N} 个 token id：\")\n",
    "    for i in sorted_indices:\n",
    "        token_id = i if i != 128000 else \"None\"\n",
    "        print(f\"{token_id}\\t{component[i]:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089574e-4879-473c-8b6b-5f24b63dd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import tempfile\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# 设置一个 ASCII-only 的临时路径\n",
    "ascii_temp_dir = \"D:/joblib_temp\"\n",
    "os.makedirs(ascii_temp_dir, exist_ok=True)\n",
    "\n",
    "# 设置 joblib 临时文件夹\n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = ascii_temp_dir\n",
    "tempfile.tempdir = ascii_temp_dir\n",
    "\n",
    "\n",
    "# 假设 svd 是你先前从 TruncatedSVD 拟合后返回的对象\n",
    "# 将前6个主成分作为特征\n",
    "\n",
    "X_proj_np = X @ svd.components_[:6].T  # shape = (n_samples, 6)\n",
    "y_np = y\n",
    "\n",
    "# —— 定义模型与参数网格 —— \n",
    "param_grid = {'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'penalty': ['l2'], 'solver': ['saga']}\n",
    "base_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# —— 拟合 GridSearch —— \n",
    "print(\"开始在 CPU 上训练（使用前 6 个主成分） …\")\n",
    "grid.fit(X_proj_np, y_np)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "print(\" Best params:\", grid.best_params_)\n",
    "print(\"交叉验证最佳准确率：\", grid.best_score_)\n",
    "\n",
    "# —— 默认阈值 0.5 下评估 —— \n",
    "y_pred_05 = best_model.predict(X_proj_np)\n",
    "print(\"\\n[默认阈值 0.5] Accuracy:\", accuracy_score(y_np, y_pred_05))\n",
    "print(classification_report(y_np, y_pred_05, zero_division=0))\n",
    "\n",
    "# —— 阈值搜索 —— \n",
    "probs = best_model.predict_proba(X_proj_np)[:, 1]\n",
    "thresholds = np.arange(0.0, 1.0 + 0.001, 0.001)\n",
    "\n",
    "best_t, best_acc = 0.5, 0.0\n",
    "for t in thresholds:\n",
    "    y_pred_t = (probs >= t).astype(int)\n",
    "    acc = accuracy_score(y_np, y_pred_t)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_t = t\n",
    "\n",
    "print(f\" 最优阈值（按 Accuracy 最大化，步长=0.001）：{best_t:.3f}，对应 Accuracy={best_acc:.4f}\")\n",
    "\n",
    "# —— 最优阈值评估 —— \n",
    "y_pred_opt = (probs >= best_t).astype(int)\n",
    "\n",
    "acc_opt   = accuracy_score(y_np, y_pred_opt)\n",
    "f1_opt    = f1_score(y_np, y_pred_opt, zero_division=0)\n",
    "auc_score = roc_auc_score(y_np, probs)\n",
    "\n",
    "print(\"\\n 最终指标（在最优阈值下，全体样本）：\")\n",
    "print(f\"Accuracy: {acc_opt:.4f}\")\n",
    "print(f\"F1-score: {f1_opt:.4f}\")\n",
    "print(f\"AUC:       {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e1920-74c8-4058-8da1-5e09cc830336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 svd 是已经 fit 好的 TruncatedSVD 模型\n",
    "# 提取每个主成分中绝对值最大的 token id（即列索引）\n",
    "\n",
    "num_components = 6  # 前 6 个主成分\n",
    "top_ids = []\n",
    "\n",
    "for k in range(num_components):\n",
    "    component = svd.components_[k]  # shape: (n_features,)\n",
    "    max_idx = np.argmax(np.abs(component))  # 最大权重的 token id\n",
    "    max_value = component[max_idx]\n",
    "    top_ids.append((k + 1, max_idx, max_value))\n",
    "\n",
    "# 打印每个主成分中的最大 token id 和对应权重\n",
    "print(\"每个主成分中权重最大的 token id：\")\n",
    "for pc_id, token_id, weight in top_ids:\n",
    "    print(f\" 第 {pc_id} 个主成分: token_id = {token_id}, weight = {weight:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e36ada-be30-4704-9d5c-6cc5c57322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 加载本地 tokenizer.json 文件\n",
    "tokenizer_llama = PreTrainedTokenizerFast(tokenizer_file=\"E:/token/llama_tokenizer/llama_tokenizer/tokenizer.json\")\n",
    "\n",
    "# 获取 vocab size\n",
    "print(\"词表长度（vocab size）：\", tokenizer_llama.vocab_size)\n",
    "\n",
    "# 获取词表内容，按 id 顺序排列成 list\n",
    "vocab_dict = tokenizer_llama.get_vocab()\n",
    "sorted_vocab = sorted(vocab_dict.items(), key=lambda x: x[1])\n",
    "vocab_list = [token for token, _ in sorted_vocab]\n",
    "\n",
    "#  vocab_list 就是 id 对应的 token 列表\n",
    "print(\"前 10 个 token 示例：\", vocab_list[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7bb870-6c1e-4eec-8190-7c68451da9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 vocab_list 是 tokenizer 的词表，长度等于 svd.components_.shape[1]\n",
    "# top_token_ids 是你从每个主成分中提取出的最大 token id\n",
    "\n",
    "top_token_ids = [128000, 60, 13, 570, 7966, 8]  # 示例值（来自前面代码提取）\n",
    "\n",
    "# 如果你希望 token_id=128000 特殊标记为 'non'\n",
    "mapped_tokens = [\"None\" if tid == 128000 else vocab_list[tid] for tid in top_token_ids]\n",
    "\n",
    "# 打印对应的 token\n",
    "for tid, token in zip(top_token_ids, mapped_tokens):\n",
    "    print(f\"token_id = {tid}, token = {token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e6dfa-fd54-414e-a2b0-c9e823fd26eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## KL散度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd389c-c8b8-4e3f-9bbb-8fde5f6a9eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 假设 X, X_1, X_0 都已经是概率分布（softmax 后）\n",
    "# 并且它们都是 torch.tensor 类型，shape = [N, vocab_size]\n",
    "\n",
    "# Step 1: 构造参考分布（预测正确样本的平均概率分布）\n",
    "mean_dist = X_1.mean(dim=0)         # shape: [vocab_size]\n",
    "mean_dist = mean_dist + 1e-12       # 避免除以 0\n",
    "mean_dist = mean_dist / mean_dist.sum()  # 再归一化确保为概率分布\n",
    "\n",
    "# Step 2: 批量 KL 散度函数（P 与 Q 都是概率分布）\n",
    "def batch_kl_divergence(P, Q, eps=1e-12):\n",
    "    P = P + eps\n",
    "    P = P / P.sum(dim=1, keepdim=True)  # 归一化（如果略有偏离）\n",
    "    Q = Q + eps\n",
    "    Q = Q / Q.sum()                     # 确保参考分布也归一化\n",
    "    Q = Q.unsqueeze(0).expand_as(P)     # 扩展成与 P 相同 shape\n",
    "    return (P * (P / Q).log()).sum(dim=1)\n",
    "\n",
    "# Step 3: 分别计算 KL 散度\n",
    "kl_all = batch_kl_divergence(X, mean_dist)\n",
    "kl_pos = batch_kl_divergence(X_1, mean_dist)\n",
    "kl_neg = batch_kl_divergence(X_0, mean_dist)\n",
    "\n",
    "# Step 4: 可视化对比分布\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(kl_pos.cpu().numpy(), bins=50, alpha=0.6, label='Correct Samples', color='#6D749E')\n",
    "plt.hist(kl_neg.cpu().numpy(), bins=50, alpha=0.6, label='Incorrect Samples', color='#D8B7A6')\n",
    "plt.xlabel(\"KL Divergence to Reference (Correct Avg)\")\n",
    "plt.ylabel(\"Sample Count\")\n",
    "plt.title(\"KL Divergence Distributions: Correct vs Incorrect\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "#  保存图像\n",
    "output_path = \"E:/tongji/math500_llama_KL_divergence.jpg\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ad290-b7de-446f-a91e-6cac986bc610",
   "metadata": {},
   "source": [
    "## TOP-k token分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd64487-7db1-4e3f-9025-9d6be8e0b89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def topk_from_mean_distribution(X_tensor, k=10):\n",
    "    \"\"\"\n",
    "    对输入的 softmax 后概率矩阵求平均分布，然后取 Top-K token。\n",
    "    :param X_tensor: torch.Tensor，shape = [N, vocab_size]，每行是一个样本的概率分布\n",
    "    :param k: 取 Top-K\n",
    "    :return: token_id 和对应的平均概率值\n",
    "    \"\"\"\n",
    "    mean_dist = X_tensor.mean(dim=0)  # shape = [vocab_size]\n",
    "    topk_probs, topk_ids = torch.topk(mean_dist, k=k, dim=0)\n",
    "    return topk_ids, topk_probs\n",
    "\n",
    "#  对预测正确的样本 X_1 做分析\n",
    "topk_ids_1, topk_probs_1 = topk_from_mean_distribution(X_1, k=10)\n",
    "print(\" 预测正确样本（X_1）的平均分布 Top-K token：\")\n",
    "for i in range(10):\n",
    "    print(f\"  token_id = {topk_ids_1[i].item():6d}, prob = {topk_probs_1[i].item():.6f}\")\n",
    "\n",
    "#  对预测错误的样本 X_0 做分析\n",
    "topk_ids_0, topk_probs_0 = topk_from_mean_distribution(X_0, k=10)\n",
    "print(\"\\n 预测错误样本（X_0）的平均分布 Top-K token：\")\n",
    "for i in range(10):\n",
    "    print(f\"  token_id = {topk_ids_0[i].item():6d}, prob = {topk_probs_0[i].item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a1cb5-875a-4a0f-952d-fd19c5bdfbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 转为 numpy\n",
    "topk_ids_1_np = topk_ids_1.cpu().numpy()\n",
    "topk_probs_1_np = topk_probs_1.cpu().numpy()\n",
    "topk_ids_0_np = topk_ids_0.cpu().numpy()\n",
    "topk_probs_0_np = topk_probs_0.cpu().numpy()\n",
    "\n",
    "# 创建画布\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# 子图 1：X_1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(10), topk_probs_1_np, tick_label=topk_ids_1_np,color='#6D749E')\n",
    "plt.title(\"Top-K Tokens (Correct Samples)\")\n",
    "plt.xlabel(\"Token ID\")\n",
    "plt.ylabel(\"Mean Probability\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# 子图 2：X_0\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(10), topk_probs_0_np, tick_label=topk_ids_0_np, color='#D8B7A6')\n",
    "plt.title(\"Top-K Tokens (Incorrect Samples)\")\n",
    "plt.xlabel(\"Token ID\")\n",
    "plt.ylabel(\"Mean Probability\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"E:/tongji/math500_topk_comparison.jpg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969ae7b-3edb-44c5-a089-c94c64007683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_ids = [128000, 60, 13, 570, 7966, 8, 92, 627, 13244, 334,\n",
    "             128000, 284, 60, 8, 11, 13, 61, 382, 17, 482]\n",
    "\n",
    "tokens = [\"none\" if i == 128000 else vocab_list[i] for i in token_ids]\n",
    "\n",
    "# 打印结果\n",
    "for tid, token in zip(token_ids, tokens):\n",
    "    print(f\"ID {tid}: {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012eb918-e8d5-4d06-903b-d09975080264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topk_probs(probs_np, vocab, k=10):\n",
    "    \"\"\"\n",
    "    输入已经是 softmax 概率矩阵。\n",
    "    \"\"\"\n",
    "    # 如果传入是 numpy，就转 tensor；如果已经是 tensor 就直接用\n",
    "    if isinstance(probs_np, np.ndarray):\n",
    "        probs = torch.from_numpy(probs_np).float()\n",
    "    elif isinstance(probs_np, torch.Tensor):\n",
    "        probs = probs_np.float()\n",
    "    else:\n",
    "        raise TypeError(\"输入必须是 numpy.ndarray 或 torch.Tensor\")\n",
    "\n",
    "    #  不再 softmax，直接 topk\n",
    "    topk_probs, topk_indices = torch.topk(probs, k, dim=1)\n",
    "\n",
    "    # 转换为 token 字符串\n",
    "    topk_token_strs = []\n",
    "    for row in topk_indices:\n",
    "        row_strs = []\n",
    "        for token_id in row.tolist():\n",
    "            if token_id < len(vocab):\n",
    "                row_strs.append(vocab[token_id])\n",
    "            elif token_id == 128000:\n",
    "                row_strs.append(\"<None>\")\n",
    "            elif token_id == 128001:\n",
    "                row_strs.append(\"<placeholder-1>\")\n",
    "            elif token_id == 128002:\n",
    "                row_strs.append(\"<placeholder-2>\")\n",
    "            elif token_id == 128003:\n",
    "                row_strs.append(\"<placeholder-3>\")\n",
    "            else:\n",
    "                row_strs.append(\"<unk>\")\n",
    "        topk_token_strs.append(row_strs)\n",
    "\n",
    "    return topk_indices.numpy(), topk_probs.numpy(), topk_token_strs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa77bf9-b3ba-4c08-96bf-e22d5f098f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你已经有：X, X_1, X_0 是 numpy 的 logits 数组\n",
    "# vocab 是 list: vocab[token_id] = \"some_word\"\n",
    "k = 10\n",
    "\n",
    "topk_ids_all, topk_probs_all, topk_strs_all = extract_topk_probs(X, vocab_list, k)\n",
    "topk_ids_pos, topk_probs_pos, topk_strs_pos = extract_topk_probs(X_1, vocab_list, k)\n",
    "topk_ids_neg, topk_probs_neg, topk_strs_neg = extract_topk_probs(X_0, vocab_list, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9138dd-0ac3-40c3-adb4-c892d55da9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 所有 top-1 token（每行第一个）\n",
    "top1_all = [row[0] for row in topk_strs_all]\n",
    "top1_pos = [row[0] for row in topk_strs_pos]\n",
    "top1_neg = [row[0] for row in topk_strs_neg]\n",
    "\n",
    "counts_all = Counter(top1_all)\n",
    "counts_pos = Counter(top1_pos)\n",
    "counts_neg = Counter(top1_neg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
