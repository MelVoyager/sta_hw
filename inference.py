#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Run GSM8K inference with distilled Qwen2 (1.5 B) using ğŸï¸ Accelerate.
"""
import os, json, torch
from functools import partial
from pathlib import Path
from accelerate.utils import set_seed
from tqdm import tqdm

os.environ["HF_DATASETS_CACHE"] = "/gpfs/junlab/xiazeyu21/Datasets"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

MODEL_DIR = Path("/gpfs/junlab/xiazeyu21/Models/R1-Distill-Qwen-7B")
BATCH_SIZE = 32
MAX_PROMPT_LEN = 128
MAX_NEW_TOKENS = 128
SEED = 42

from accelerate import Accelerator
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoConfig,
    GenerationConfig,
)

# ---------- 0. ä¸€åŠ³æ°¸é€¸ä¿®è¡¥ configï¼ˆåªåœ¨é¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰ ----------
# cfg_path = MODEL_DIR / "config.json"
# cfg = AutoConfig.from_pretrained(MODEL_DIR, trust_remote_code=True)
# updated = False
# if getattr(cfg, "num_key_value_heads", None) != 2:
#     cfg.num_key_value_heads = 2
#     cfg.multi_query = True
#     cfg.save_pretrained(MODEL_DIR)
#     updated = True
#     print("âœ” å·²ä¿®è¡¥ config.json çš„ KV-heads=2")

# ---------- 1. åˆå§‹åŒ– Accelerator ----------
accelerator = Accelerator()
accelerator.print(f"ğŸ Accelerator initialised on {accelerator.device}")
set_seed(SEED)
# ---------- 2. åŠ è½½ tokenizer / model ----------
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True, use_fast=False)
tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16,
    device_map="auto",          # è®© HF åˆ†å¡ï¼ŒAccelerate åªå¤„ç† sync
    trust_remote_code=True,
)

# ï¼ˆå¯é€‰ï¼‰åªåœ¨å•å¡åœºæ™¯å°è¯• compile
if torch.cuda.device_count() == 1:
    model = torch.compile(model)

# å°†æ¨¡å‹äº¤ç»™ acceleratorï¼›ä¿æŒ tokenizer åœ¨ CPU å³å¯
# model = accelerator.prepare(model)

# ---------- 3. æ•°æ® ----------------------------------------------------------------
ds = load_dataset("gsm8k", "main", split="test").select(range(320))

def build_prompt(q: str) -> str:
    return f"Q: {q.strip()}\nA:"

def collate(batch, tok, max_len):
    prompts = [build_prompt(x["question"]) for x in batch]
    enc = tok(prompts,
              padding="max_length",
              max_length=max_len,
              truncation=True,
              return_tensors="pt")
    return enc, prompts

loader = DataLoader(
    ds, batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=partial(collate, tok=tokenizer, max_len=MAX_PROMPT_LEN)
)

loader = accelerator.prepare(loader)  # è®© dataloader ä¹Ÿèƒ½å¤šå¡åŒæ­¥

# ---------- 4. generation config ---------------------------------------------------
gen_cfg = GenerationConfig(
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    max_new_tokens=MAX_NEW_TOKENS,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
)

# ---------- 5. æ¨ç†å¾ªç¯ ------------------------------------------------------------
model.eval()
all_out = []

with torch.no_grad():
    for step, (batch_tok, raw_prompts) in tqdm(enumerate(loader), total=len(loader)):
        # batch_tok å·²åœ¨æ­£ç¡®è®¾å¤‡ï¼›æ— éœ€ .to(model.device)
        outputs = model.generate(**batch_tok, generation_config=gen_cfg)
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        # å‰¥æ‰ promptï¼Œåªç•™ç­”æ¡ˆ
        for full_text, prompt in zip(decoded, raw_prompts):
            ans = full_text[len(prompt):].strip()
            all_out.append(ans)
        # accelerator.print(f"âœ… batch {step} done")

# ---------- 6. æ‰“å°å‰ 10 æ¡ ---------------------------------------------------------
for i, (q, a) in enumerate(zip(ds["question"], all_out[:10]), 1):
    print(f"\nQ{i}: {q}\nA{i}: {a}\n" + "-" * 60)
