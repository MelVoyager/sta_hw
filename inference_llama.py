import os, json, torch
from functools import partial
from pathlib import Path
from accelerate.utils import set_seed
from tqdm import tqdm
import torch.nn.functional as F 

os.environ["HF_DATASETS_CACHE"] = "/gpfs/junlab/xiazeyu21/Datasets"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

MODEL_DIR = Path("/gpfs/junlab/xiazeyu21/Models/R1-Distill-Llama-8B")
BATCH_SIZE = 2
MAX_PROMPT_LEN = 512
MAX_NEW_TOKENS = 4096
SEED = 42
DATASET_SIZE = 500

from accelerate import Accelerator
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoConfig,
    GenerationConfig,
)

# ---------- 0. ä¸€åŠ³æ°¸é€¸ä¿®è¡¥ configï¼ˆåªåœ¨é¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰ ----------
# cfg_path = MODEL_DIR / "config.json"
# cfg = AutoConfig.from_pretrained(MODEL_DIR, trust_remote_code=True)
# updated = False
# if getattr(cfg, "num_key_value_heads", None) != 2:
#     cfg.num_key_value_heads = 2
#     cfg.multi_query = True
#     cfg.save_pretrained(MODEL_DIR)
#     updated = True
#     print("âœ” å·²ä¿®è¡¥ config.json çš„ KV-heads=2")

# ---------- 1. åˆå§‹åŒ– Accelerator ----------
accelerator = Accelerator()
accelerator.print(f"ğŸ Accelerator initialised on {accelerator.device}")
set_seed(SEED)
# ---------- 2. åŠ è½½ tokenizer / model ----------
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True, use_fast=False)
tokenizer.pad_token_id = tokenizer.eos_token_id

tokenizer.save_pretrained("dataset/llama_tokenizer")

model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16,
    device_map="auto",          # è®© HF åˆ†å¡ï¼ŒAccelerate åªå¤„ç† sync
    trust_remote_code=True,
)

# ï¼ˆå¯é€‰ï¼‰åªåœ¨å•å¡åœºæ™¯å°è¯• compile
if torch.cuda.device_count() == 1:
    model = torch.compile(model)

# å°†æ¨¡å‹äº¤ç»™ acceleratorï¼›ä¿æŒ tokenizer åœ¨ CPU å³å¯
# model = accelerator.prepare(model)

# ---------- 3. æ•°æ® ----------------------------------------------------------------
# ds = load_dataset("gsm8k", "main", split="test")
dataset_path = "/gpfs/junlab/xiazeyu21/Datasets/math-500"
ds = load_dataset(dataset_path)
ds = ds["test"]
ds = ds.select(range(min(len(ds), DATASET_SIZE)))
print(f'dataset size:{len(ds)}')

def build_prompt(q: str) -> str:
    return f"""You are a helpful and harmless assistant. You should think step-by-step.
Question: {q.strip()}\n
Here is your answer:\n"""

def collate(batch, tok, max_len):
    prompts = [build_prompt(x["problem"]) for x in batch]
    enc = tok(prompts,
              padding="max_length",
              max_length=max_len,
              truncation=True,
              return_tensors="pt")
    # length = tok(prompts,
    #             padding="max_length",
    #             max_length=max_len,
    #             truncation=False,
    #             return_length=True)["length"]
    # print(f"max length:{max(length)}")
    return enc, prompts

loader = DataLoader(
    ds, batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=partial(collate, tok=tokenizer, max_len=MAX_PROMPT_LEN)
)

loader = accelerator.prepare(loader)  # è®© dataloader ä¹Ÿèƒ½å¤šå¡åŒæ­¥


# ---------- 4. generation config ---------------------------------------------------
gen_cfg = GenerationConfig(
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    max_new_tokens=MAX_NEW_TOKENS,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
)

# ---------- 5. æ¨ç†å¾ªç¯ ------------------------------------------------------------
model.eval()
ans_list      = []            # ç­”æ¡ˆæ–‡æœ¬
prob_chunks   = []            # æ¯ä¸ª batch çš„ [bs, vocab] æ¦‚ç‡

with torch.no_grad():
    for step, (batch_tok, raw_prompts) in tqdm(enumerate(loader), total=len(loader)):
        outputs = model.generate(
            **batch_tok,
            generation_config=gen_cfg,
            output_scores=False,
            return_dict_in_generate=True
        )

        # 1ï¸âƒ£ å–æœ€åä¸€æ­¥ logits â†’ prob
        logits_last = model(
            outputs.sequences[:, -1:],  # åªå–‚æœ€åä¸€ä¸ª token
            past_key_values=None,
        ).logits[:, -1, :]             # [B, vocab]
        probs_last = F.softmax(logits_last.float(), dim=-1)
        probs_gather = accelerator.gather_for_metrics(probs_last)  # æ±‡æ€»å„è¿›ç¨‹
        prob_chunks.append(probs_gather.cpu())               # ç§»å› CPUï¼ŒèŠ‚çœæ˜¾å­˜

        # 2ï¸âƒ£ ä¿å­˜è§£ç æ–‡æœ¬ï¼ˆå’ŒåŸæ¥ä¸€æ ·ï¼‰
        decoded = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)
        for full_text, prompt in zip(decoded, raw_prompts):
            ans_list.append(full_text[len(prompt):].strip())

# ---------- 6. æ‹¼æ¥å¾—åˆ° [dataset_size, vocab] --------------------------------------
all_step_probs = torch.cat(prob_chunks, dim=0)[:len(ds)]  # ä¸‡ä¸€å¤šå¡æœ‰ padding
print("probs shape =", all_step_probs.shape)
result = (ans_list, all_step_probs)


torch.save(result, f"/home/xiazeyu21/sta_hw/dataset/math500_{len(ds)}_llama.pt")
# ---------- 6. æ‰“å°å‰ 10 æ¡ ---------------------------------------------------------
# for i, (q, a) in enumerate(zip(ds["question"], ans_list[:10]), 1):
#     print(f"\nQ{i}: {q}\nA{i}: {a}\n" + "-" * 60)